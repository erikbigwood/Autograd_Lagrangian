@article{Dirac1933,
   author = {P A M Dirac},
   journal = {Physikalische Zeitschrift der Sowjetunion},
   pages = {64-72},
   title = {The Lagrangian in Quantum Mechanics},
   volume = {3},
   year = {1933}
}
@article{Cranmer2020,
   abstract = {Accurate models of the world are built upon notions of its underlying symmetries. In physics, these symmetries correspond to conservation laws, such as for energy and momentum. Yet even though neural network models see increasing use in the physical sciences, they struggle to learn these symmetries. In this paper, we propose Lagrangian Neural Networks (LNNs), which can parameterize arbitrary Lagrangians using neural networks. In contrast to models that learn Hamiltonians, LNNs do not require canonical coordinates, and thus perform well in situations where canonical momenta are unknown or difficult to compute. Unlike previous approaches, our method does not restrict the functional form of learned energies and will produce energy-conserving models for a variety of tasks. We test our approach on a double pendulum and a relativistic particle, demonstrating energy conservation where a baseline approach incurs dissipation and modeling relativity without canonical coordinates where a Hamiltonian approach fails. Finally, we show how this model can be applied to graphs and continuous systems using a Lagrangian Graph Network, and demonstrate it on the 1D wave equation.},
   author = {Miles Cranmer and Sam Greydanus and Stephan Hoyer and Peter Battaglia and David Spergel and Shirley Ho},
   month = {5},
   title = {Lagrangian Neural Networks},
   url = {https://arxiv.org/pdf/2003.04630.pdf},
   year = {2020}
}
@book{Taylor2005,
   author = {John R Taylor},
   city = {Mill Valley, CA},
   edition = {1},
   editor = {Lee Young},
   publisher = {University Science Books},
   title = {Classical Mechanics},
   year = {2005}
}
@misc{,
   author = {Pytorch},
   title = {torch.vmap},
   url = {https://pytorch.org/docs/stable/generated/torch.vmap.html}
}
@misc{,
   author = {Pytorch},
   title = {torch.func.jacfwd},
   url = {https://pytorch.org/docs/stable/generated/torch.func.jacfwd.html#torch.func.jacfwd}
}
@article{Paszke2019,
   abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
   author = {Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury Google and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas KÃ¶pf Xamla and Edward Yang and Zach Devito and Martin Raison Nabla and Alykhan Tejani and Sasank Chilamkurthy and Qure Ai and Benoit Steiner and Lu Fang Facebook and Junjie Bai Facebook and Soumith Chintala},
   title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
   year = {2019}
}
